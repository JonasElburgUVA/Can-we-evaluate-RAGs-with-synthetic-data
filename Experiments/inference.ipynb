{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Buddy Inference\n",
    "This notebook makes predictions on a dataset of question-answer pairs using the provided knowledge buddies. The answers are then evaluated using the reference answers. Next, we generate synthetic question-answer pairs based on the corpus, and repeat the same experiment. The choice of metric(s) is customisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"sales_five_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "root = os.environ[\"PROJECT_ROOT\"]\n",
    "sys.path.append(root)\n",
    "import pdstools.infinity.client as client\n",
    "import random\n",
    "from utils.aync_question import question_async\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from utils.utils_eval import eval_dataset\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from utils.dataset import MultiDocDataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join(root, \"configs\", \"experiments\", f\"{config}.yaml\"), \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "num_gen_questions_per_document = config.get(\"num_gen_questions_per_document\")\n",
    "num_source_docs = config.get(\"num_source_docs\")\n",
    "num_human_questions = config.get(\"num_human_questions\", num_gen_questions_per_document * num_source_docs)\n",
    "\n",
    "cl_user = client.Infinity.from_basic_auth(\n",
    "    pega_version='24.2',\n",
    "    timeout=100000,\n",
    "    user_name=os.environ[\"PEGA_USERNAME\"],\n",
    "    password=os.environ[\"PEGA_PASSWORD\"])\n",
    "\n",
    "cl_user.knowledge_buddy.question_async = partial(question_async, self=cl_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Human dataset\n",
    "We first make predictions on the human-annotated data. We start by loading the human annotated questions:\n",
    "\n",
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_kwargs = {}\n",
    "\n",
    "ds = MultiDocDataSet(\n",
    "    name=config[\"dataset_name\"],\n",
    "    experiment_name = config['experiment_name'],\n",
    "    **ds_kwargs\n",
    ")\n",
    "\n",
    "ds.load_human_questions(N=num_human_questions, use_doc_ids=config.get(\"use_doc_ids\", False))\n",
    "\n",
    "data_descriptives = ds.get_descriptives()\n",
    "print(f\"Descriptives of the {config[\"dataset_name\"]} dataset:\")\n",
    "pd.DataFrame.from_dict(data_descriptives, orient=\"index\", columns=[\"Value\"]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_descriptives = ds.get_question_descriptives()\n",
    "print(f\"Descriptives of the human-annotated QA pairs of the {config[\"dataset_name\"]} dataset:\")\n",
    "pd.DataFrame.from_dict(question_descriptives, orient=\"index\", columns=[\"Value\"]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask the questions to the buddies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for buddy in config[\"buddies\"]:\n",
    "    print(buddy)\n",
    "    try:\n",
    "        await ds.predict_and_save(\n",
    "            knowledge_buddy_client=cl_user,\n",
    "            buddy_name=buddy,\n",
    "            include_search_results=True,\n",
    "            allow_overwrite=False,\n",
    "            batching=10\n",
    "        )\n",
    "    except FileExistsError:\n",
    "        print(f\"predictions for the {buddy} buddy have already been made. Set overwrite to true or create a new experiment under a different name to make new predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We now evaluate the answers by comparing them with the reference answers using the defined metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, simple_results = eval_dataset(ds, config, overwrite=False, upload=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(simple_results).round(3)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data\n",
    "We now repeat this experiment with synthetic data. Adjust the prompt as you like! Just note the prompt keys ('context' and 'num_questions_per_documents' in the default prompt), which should match the prompt when generating the questions.\n",
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "deployment = config[\"generator_llm_deployment\"]\n",
    "prompt_keys = ['context', 'num_questions_per_document']\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"\\\n",
    "                Context information is below..\n",
    "\n",
    "                -----------------------------\n",
    "                {context}\n",
    "                -----------------------------\n",
    "\n",
    "                Given the context information and not prior knowledge.\n",
    "                Generate only questions based on the below query.\n",
    "\n",
    "                You are a Professor. Your task is to setup \\\n",
    "                {num_questions_per_document} questions for an upcoming \\\n",
    "                quiz/examination. The questions should be diverse in nature \\\n",
    "                across the document. The questions should not contain options, not start with Q1/Q2. \\\n",
    "                Restrict the questions to the context information provided.\\\n",
    "                Provide the correct answers together with the questions in json format, using 'question' and 'reference' as keys.\\\n",
    "                Make sure you fact check your work.\n",
    "            \"\"\"\n",
    "        )\n",
    "    ]\n",
    ");\n",
    "\n",
    "num_questions_per_document = config[\"num_gen_questions_per_document\"]\n",
    "\n",
    "prompt_args = {\n",
    "    \"num_questions_per_document\" : num_questions_per_document\n",
    "    # ADD ADDITIONAL PROMPT ARGS (except for 'context') HERE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.init_generator(\n",
    "    deployment=deployment,\n",
    "    prompt=prompt,\n",
    "    prompt_keys=prompt_keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds.generate_and_save(\n",
    "        prompt_args = prompt_args,\n",
    "        data_type= config[\"generated_dataset_name\"],\n",
    "        allow_overwrite=False, # keep at false to not simulate data unnecessarily\n",
    "        num_source_docs = config[\"num_source_docs\"],\n",
    "        min_doc_length=config.get(\"min_source_doc_length\", 0)\n",
    "    )\n",
    "    ds.load_generated_questions(data_type=config[\"generated_dataset_name\"], use_doc_ids=True)\n",
    "except FileExistsError:\n",
    "    print(\"These questions were already generated.\")\n",
    "    ds.load_generated_questions(data_type=config[\"generated_dataset_name\"], use_doc_ids=True);\n",
    "\n",
    "question_descriptives = {k:[v] for k,v in ds.get_question_descriptives().items()}\n",
    "pd.DataFrame(question_descriptives).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for buddy in config[\"buddies\"]:\n",
    "    print(f\"asking {buddy}...\")\n",
    "    try:\n",
    "        await ds.predict_and_save(knowledge_buddy_client=cl_user, \n",
    "                        buddy_name=buddy, \n",
    "                        include_search_results=True, \n",
    "                        allow_overwrite=False,\n",
    "                        batching=10) \n",
    "    except FileExistsError:\n",
    "        print(f\"Predictions have already been made with the {buddy} buddy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, simple_results = eval_dataset(ds, config, overwrite=False, upload=False)\n",
    "df = pd.DataFrame(simple_results).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
