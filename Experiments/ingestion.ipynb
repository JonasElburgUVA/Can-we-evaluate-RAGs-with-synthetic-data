{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root = os.environ['PROJECT_ROOT']\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import pdstools.infinity.client as client\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "cl = client.Infinity.from_basic_auth(\n",
    "    pega_version='24.2',\n",
    "    timeout=1000000,\n",
    "    user_name=os.environ['PEGA_USERNAME']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to ingest a corpus into a specified datasource. We do this by first loading a template json request file, where we update the necessary variables and attributes. The structure of such an ingestion request can be found in the [pega documentation](https://docs.pega.com/bundle/knowledge-buddy/page/knowledge-buddy/implementation/ingestion-api.html). Before you attempt to run this notebook, ensure you have done preprocessing properly, as is documented in the pp_template.ipynb notebook.\n",
    "\n",
    "These are the only variables you need to change to customise your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Sales\"\n",
    "collection = \"Experiments\"\n",
    "ingestion_template_path = f\"{root}/data/ingestion/ingestion_single_doc.json\"\n",
    "chunk_overlap = 200 #chars\n",
    "chunk_size = 1000 #chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create requests\n",
    "Using the corpus and ingestion template, we create a request for every document in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(root, \"data\", dataset_name, \"corpus.json\"), \"r\", encoding=\"utf8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# filter out title only documents\n",
    "corpus = {k:v for (k,v) in corpus.items() if k!=v}\n",
    "\n",
    "with open(ingestion_template_path, \"rb\") as f:\n",
    "    template = json.load(f)\n",
    "\n",
    "ingestions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (title, content) in enumerate(corpus.items()):\n",
    "    ingestion = copy.deepcopy(template)\n",
    "    ingestion[\"text\"][0][\"content\"] = content\n",
    "    ingestion[\"dataSource\"] = dataset_name\n",
    "    ingestion[\"title\"] = title\n",
    "    ingestion[\"collection\"] = collection\n",
    "    ingestion[\"objectId\"] = title\n",
    "    ingestion[\"chunkSize\"] = chunk_size\n",
    "    ingestion[\"chunkOverlap\"] = chunk_overlap\n",
    "\n",
    "    ingestions.append(ingestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a quick plot to get some information about the data. If you have any documents that are particularly large (anecdotally >15.000 words), the ingestion may fail. In that case you may want to split up these documents into multiple requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get descriptives\n",
    "texts_lengths = [len(t.split()) for t in corpus.values()]\n",
    "m, sd = np.mean(texts_lengths), np.std(texts_lengths)\n",
    "print(f\"Mean words per doc: {m}\")\n",
    "print(f\"SD words per doc: {sd}\")\n",
    "print(f\"Total docs: {len(corpus)}\")\n",
    "print(f\"Total words in corpus: {sum(texts_lengths)}\")\n",
    "\n",
    "sns.histplot(data=texts_lengths)\n",
    "plt.xlabel(\"Document length (words)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion - ONLY RUN ONCE\n",
    "This code ingests the corpus into the specified dataSource. Only do this once to prevent duplicate documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN THIS ONCE!\n",
    "for request in tqdm(ingestions):\n",
    "    response = cl.put(\n",
    "        data=request,\n",
    "        endpoint= os.environ[\"PEGA_BASE_URL\"]+\"indexes\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
