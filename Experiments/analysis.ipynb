{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "This notebook is aimed at analysing a human compared to a synthetic benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = \"sales_context\"\n",
    "config = \"sales_five_models\"\n",
    "# config = \"squad_five_models\"\n",
    "# config = \"ASQA_five_models\"\n",
    "# config = \"ASQA_context\"\n",
    "# config = \"Launchpad_context\"\n",
    "# config = \"Launchpad_five_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "root = os.environ.get(\"PROJECT_ROOT\")\n",
    "sys.path.append(root)\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.utils_eval import *\n",
    "from functools import partial\n",
    "from scipy.stats import wilcoxon\n",
    "import yaml\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(root, \"configs\", \"experiments\", f\"{config}.yaml\"), \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "data_types = [\"Human\", config[\"generated_dataset_name\"]]\n",
    "output_path = os.path.join(root, \"output\", config[\"dataset_name\"], config[\"experiment_name\"])\n",
    "output_path_figures = os.path.join(output_path, \"figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results, all_results_simple = load_results(\n",
    "    output_path=output_path,\n",
    "    data_types=data_types,\n",
    "    buddies=config['buddies']\n",
    ")\n",
    "df_human = pd.DataFrame.from_dict(all_results[data_types[0]], orient=\"columns\")\n",
    "df_gen = pd.DataFrame.from_dict(all_results[data_types[1]], orient=\"columns\")\n",
    "for model in config[\"buddies\"]:\n",
    "    df_human.loc[\"false_negatives\", model] = all_results_simple[data_types[0]][model][\"false_negatives\"]\n",
    "    df_gen.loc[\"false_negatives\", model] = all_results_simple[data_types[1]][model][\"false_negatives\"]\n",
    "df_comb = combine_results(df_human=df_human, df_gen=df_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = {\n",
    "    \"bleu_score\" : \"BLEU\",\n",
    "    \"rouge_score(mode=fmeasure)\": \"ROUGE\",\n",
    "    \"rouge_score\" : \"ROUGE\",\n",
    "    \"non_llm_string_similarity\": \"Lev. Dist.\",\n",
    "    \"false_negatives\" : \"FN Ratio\",\n",
    "    \"semantic_similarity\" : \"Semantic Similarity\",\n",
    "    \"string_present\" : \"String Presence\",\n",
    "    'llm_context_precision_without_reference' : \"context precision\",\n",
    "    'context_recall' : \"context recall\",\n",
    "    'faithfulness' : \"faithfulness\", \n",
    "    'answer_relevancy': \"answer relevancy\",\n",
    "}\n",
    "\n",
    "dataset_names = {\n",
    "    \"Generated\" : \"Synthetic\"\n",
    "}\n",
    "\n",
    "# df_comb = df_comb.rename(index=model_names, level=\"Model\") ADD RENAMING DICT FOR MODEL NAMES IF YOU WANT\n",
    "df_comb = df_comb.rename(index=metric_names, level=\"Metric\")\n",
    "df_comb = df_comb.rename(index=dataset_names, level=\"Dataset\")\n",
    "metrics = set(df_comb.index.get_level_values(level=\"Metric\"))\n",
    "models =  set(df_comb.index.get_level_values(level=\"Model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_sim_ecdf = plot_ecdf_metric(\n",
    "    metric=\"Semantic Similarity\",\n",
    "    df=df_comb,\n",
    "    dataset=config[\"dataset_name\"],\n",
    "    output_path=output_path_figures,\n",
    "    save=False,\n",
    "    dpi=1000\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics\n",
    "eval_metrics = {\n",
    "    m for m in metrics if m not in [\"reference\", \"response\", \"user_input\", \"retrieved_contexts\", \"FN ratio\"]\n",
    "}\n",
    "df_comb_filtered = df_comb.drop(level=\"Metric\", index=metrics-eval_metrics)\n",
    "df_comb_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb_means = df_comb_filtered.groupby(level=[\"Dataset\", \"Metric\", \"Model\"]).mean()\n",
    "df_comb_stds = df_comb_filtered.groupby(level=[\"Dataset\", \"Metric\", \"Model\"]).std()\n",
    "df_comb_means[\"Rank\"] = df_comb_means.groupby([\"Dataset\", \"Metric\"])['Value'].rank(ascending=False, method=\"dense\")\n",
    "# df_comb_means.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kendall = get_kendall_stats(df=df_comb_means)\n",
    "df_kendall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kendall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any NaN kendall tau value (all models scored the same)\n",
    "df_comb_means.drop(index=\"String Presence\", level=\"Metric\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = config[\"baseline\"]\n",
    "df = df_comb_means.groupby(['Dataset', 'Metric'], group_keys=False).apply(partial(compute_percentual_change, baseline_model=baseline_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot differences\n",
    "models = config['buddies'].copy()\n",
    "models.remove(config['baseline'])\n",
    "plot_percentual_change(df=df, models=models, output_path=output_path_figures, dpi=1000, save=False, log_scale=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
