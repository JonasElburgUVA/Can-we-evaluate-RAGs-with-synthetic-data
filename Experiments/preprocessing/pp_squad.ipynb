{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the SQuAD dataset\n",
    "First we make sure the necessary subfolders are in place:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the necessary subfolders...\n",
    "import os\n",
    "root = os.environ['PROJECT_ROOT']\n",
    "os.makedirs(os.path.join(root, \"data\", \"SQuAD\", \"source_documents\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(root, \"data\", \"SQuAD\", \"human\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "When you download the SQuAD-2.0 dataset from [GitHub](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/), you get two files:\n",
    "-`dev-v2.0.json`\n",
    "-`train-v2.0.json`\n",
    "\n",
    "These files should be in the directory `data/SQuAD/source_documents`\n",
    "\n",
    "This notebook explains extracts `questions.json` and `corpus.json` from these files in the structure expected by the rest of the code. In this case we sample the questions from the dev set, but we include documents from both dev and train splits in the corpus.\n",
    "Adjust your sampling strategy by changing the arguments in `configs/preprocessing/squad.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import yaml\n",
    "import warnings\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.environ['PROJECT_ROOT']\n",
    "datapath = os.path.join(root, \"data\", \"SQuAD\")\n",
    "config = \"squad\"\n",
    "with open(os.path.join(root, \"configs\", \"preprocessing\", f\"{config}.yaml\"), \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some checks\n",
    "if \"questions_splits\" not in config.keys():\n",
    "    warnings.warn(\"questions_splits argument not in config file. Using the default of only the dev split.\")\n",
    "questions_splits = config.get(\"questions_splits\", ['dev'])\n",
    "corpus_splits = config.get(\"corpus_splits\", ['dev', 'train'])\n",
    "assert questions_splits <= corpus_splits, f\"One of the question splits {questions_splits} is not included in the corpus splits {corpus_splits}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we construct and save the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "\n",
    "for split in corpus_splits:\n",
    "    with open(os.path.join(datapath, \"source_documents\", f\"{split}-v2.0.json\"), \"r\", encoding=\"utf8\") as f:\n",
    "        data = json.load(f)['data']\n",
    "        \n",
    "    # Create a dictionary mapping titles to concatenated paragraph texts\n",
    "    corpus.update({\n",
    "        \" \".join(d['title'].split(\"_\")): \"\\n \".join(par['context'] for par in d['paragraphs'])\n",
    "        for d in data\n",
    "    })\n",
    "\n",
    "with open(os.path.join(datapath, \"corpus.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(corpus, f, indent=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sample questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "questions = defaultdict(list)\n",
    "\n",
    "for split in questions_splits:\n",
    "    with open(os.path.join(datapath, \"source_documents\", f\"{split}-v2.0.json\"), \"r\", encoding=\"utf8\") as f:\n",
    "        data = json.load(f)['data']\n",
    "\n",
    "    n_source_docs = min(config.get(\"n_source_docs_per_split\", len(data)), len(data))\n",
    "    n_questions_per_doc = config.get(\"n_questions_per_doc\")\n",
    "    print(f\"Sampling {n_questions_per_doc * n_source_docs} questions from the {split} split...\")\n",
    "\n",
    "    sampled_docs = random.sample(data, n_source_docs)\n",
    "\n",
    "    for doc in sampled_docs:\n",
    "            title, paragraphs = doc['title'], doc['paragraphs']\n",
    "            title = \" \".join(title.split(\"_\"))\n",
    "            available_qas = [qa for par in paragraphs for qa in par['qas'] if not qa['is_impossible']]\n",
    "\n",
    "            # Exit strategy: if there are fewer questions than required, use all available questions\n",
    "            if len(available_qas) < n_questions_per_doc:\n",
    "                print(f\"document '{title}' only contains {len(available_qas)} questions, while {n_questions_per_doc} were requested. Using all available questions.\")\n",
    "                sampled_qas = available_qas\n",
    "            else:\n",
    "                sampled_qas = random.sample(available_qas, n_questions_per_doc)\n",
    "\n",
    "            # Store the selected unique questions\n",
    "            questions[title] = [\n",
    "                {\"question\": qa['question'], \"reference\": qa['answers'][0]['text']}\n",
    "                for qa in sampled_qas\n",
    "            ]\n",
    "\n",
    "with open(os.path.join(datapath,\"human\", \"questions.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "     json.dump(questions, f, indent=3)\n",
    "\n",
    "print(f\"Saved questions.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
